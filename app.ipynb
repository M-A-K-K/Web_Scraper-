{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# Function to attempt element location with retries using CSS selector\n",
    "def attempt_element_locate(selector, retries=5, wait_time=30):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            element = WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "            )\n",
    "            return element\n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            attempt += 1\n",
    "    raise Exception(f\"Failed to locate element with CSS Selector: {selector} after {retries} attempts\")\n",
    "\n",
    "# Function to extract text from an element using a CSS selector\n",
    "def extract_text_from_selector(css_selector, retries=5, wait_time=30):\n",
    "    try:\n",
    "        element = attempt_element_locate(css_selector, retries, wait_time)\n",
    "        return element.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"Text not found\"\n",
    "\n",
    "# Function to extract image URL using a CSS selector\n",
    "def extract_image_url_from_selector(css_selector, retries=5, wait_time=30):\n",
    "    try:\n",
    "        img_element = attempt_element_locate(css_selector, retries, wait_time)\n",
    "        return img_element.get_attribute('src')\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting image URL: {e}\")\n",
    "        return \"Image URL not found\"\n",
    "\n",
    "# Function to extract all image URLs from a parent div\n",
    "def extract_all_images_from_div(parent_css_selector, retries=5, wait_time=30):\n",
    "    try:\n",
    "        parent_div = attempt_element_locate(parent_css_selector, retries, wait_time)\n",
    "        img_elements = parent_div.find_elements(By.TAG_NAME, 'img')\n",
    "        image_urls = [img.get_attribute('src') for img in img_elements if img.get_attribute('src')]\n",
    "        return image_urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting images from div: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to download an image from a URL and save it as PNG\n",
    "def download_image(image_url, output_path):\n",
    "    try:\n",
    "        print(f\"Attempting to download image from: {image_url}\")\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(output_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"Image downloaded successfully: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "\n",
    "# Function to download a file from a URL\n",
    "def download_file(file_url, output_path):\n",
    "    try:\n",
    "        print(f\"Attempting to download file from: {file_url}\")\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(output_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"File downloaded successfully: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://www.molteni.it/ap/product/d1542'\n",
    "\n",
    "# Open the product page\n",
    "driver.get(url)\n",
    "\n",
    "# Define the CSS selectors\n",
    "h1_selector = 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-text.animation-fade-in > div > div > div.block-info-product__top.padding-line-element > a > h1'\n",
    "img_selector = 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-img > div.animation-mask.p81 > div > a > img'\n",
    "h3_selector = 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-text.animation-fade-in > div > div > h3'\n",
    "div_selector_1 = '#block-0 > div > div'\n",
    "div_selector_2 = '#block-2 > div > div'\n",
    "img_selector_2 = '#block-4 > div > div > div.col-8.block-text-img-img.is-768 > div > img'\n",
    "div_selector_3 = '#block-5 > div > div'\n",
    "file_selector = '#specs > div > div.row.product-specs-row > div:nth-child(3) > a'\n",
    "\n",
    "# Extract the <h1> text\n",
    "h1_text = extract_text_from_selector(h1_selector)\n",
    "print(f\"Extracted <h1> text: {h1_text}\")\n",
    "\n",
    "# Create directory based on <h1> text\n",
    "h1_folder_name = h1_text.replace('/', '-').replace('\\\\', '-')  # Sanitize folder name\n",
    "product_folder_path = os.path.join(h1_folder_name)\n",
    "if not os.path.exists(product_folder_path):\n",
    "    os.makedirs(product_folder_path)\n",
    "\n",
    "# Extract the image URL from the primary image selector\n",
    "img_url = extract_image_url_from_selector(img_selector)\n",
    "print(f\"Extracted image URL: {img_url}\")\n",
    "\n",
    "# Extract the <h3> text\n",
    "h3_text = extract_text_from_selector(h3_selector)\n",
    "print(f\"Extracted <h3> text: {h3_text}\")\n",
    "\n",
    "# Extract all image URLs from the specified divs\n",
    "image_urls_div_1 = extract_all_images_from_div(div_selector_1)\n",
    "print(f\"Extracted image URLs from div #block-0: {image_urls_div_1}\")\n",
    "\n",
    "image_urls_div_2 = extract_all_images_from_div(div_selector_2)\n",
    "print(f\"Extracted image URLs from div #block-2: {image_urls_div_2}\")\n",
    "\n",
    "# Extract the image URL from #block-4\n",
    "img_url_2 = extract_image_url_from_selector(img_selector_2)\n",
    "print(f\"Extracted image URL from #block-4: {img_url_2}\")\n",
    "\n",
    "# Extract all image URLs from #block-5\n",
    "image_urls_div_3 = extract_all_images_from_div(div_selector_3)\n",
    "print(f\"Extracted image URLs from div #block-5: {image_urls_div_3}\")\n",
    "\n",
    "# List all image URLs to download\n",
    "all_image_urls = [img_url] + [img_url_2] + image_urls_div_1 + image_urls_div_2 + image_urls_div_3\n",
    "print(f\"All image URLs to download: {all_image_urls}\")\n",
    "\n",
    "# Download images directly in the product folder\n",
    "for idx, img_url in enumerate(all_image_urls):\n",
    "    if img_url and img_url.startswith('http'):\n",
    "        img_file_path = os.path.join(product_folder_path, f'image_{idx + 1}.png')\n",
    "        download_image(img_url, img_file_path)\n",
    "\n",
    "# Extract file download link and download the file\n",
    "file_element = attempt_element_locate(file_selector)\n",
    "if file_element:\n",
    "    file_link = file_element.get_attribute('href')\n",
    "    if file_link and file_link.startswith('http'):\n",
    "        print(f\"File link extracted: {file_link}\")\n",
    "        download_file(file_link, os.path.join(product_folder_path, 'description.pdf'))\n",
    "    else:\n",
    "        print(\"No valid file link found.\")\n",
    "else:\n",
    "    print(\"File element not found.\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://www.molteni.it/ap/highlights'\n",
    "\n",
    "# Open the webpage\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    # Wait until the section containing the articles is loaded\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    section = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section.product-category > div > div > div > section.content-block.catalog-list')))\n",
    "    \n",
    "    # Find all article tags within the section\n",
    "    articles = section.find_elements(By.TAG_NAME, 'article')\n",
    "    \n",
    "    # Loop through each article tag and find all <a> tags inside it\n",
    "    for article in articles:\n",
    "        links = article.find_elements(By.TAG_NAME, 'a')\n",
    "        for link in links:\n",
    "            # Print the href attribute of each <a> tag (which contains the URL)\n",
    "            print(link.get_attribute('href'))\n",
    "\n",
    "except TimeoutException:\n",
    "    print(\"Loading the section took too long.\")\n",
    "except NoSuchElementException:\n",
    "    print(\"Could not find the required elements on the page.\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# URL of the page you want to scrape (all products page)\n",
    "main_url = 'https://www.molteni.it/ap/highlights'\n",
    "\n",
    "# Function to attempt element location with retries using CSS selector\n",
    "def attempt_element_locate(driver, selector, retries=5, wait_time=30):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            element = WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "            )\n",
    "            return element\n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            attempt += 1\n",
    "    raise Exception(f\"Failed to locate element with CSS Selector: {selector} after {retries} attempts\")\n",
    "\n",
    "# Function to extract all product links\n",
    "def get_product_links():\n",
    "    driver.get(main_url)\n",
    "    try:\n",
    "        # Wait until the section containing the articles is loaded\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        section = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section.product-category > div > div > div > section.content-block.catalog-list')))\n",
    "        \n",
    "        # Find all article tags within the section\n",
    "        articles = section.find_elements(By.TAG_NAME, 'article')\n",
    "        product_links = []\n",
    "        \n",
    "        # Loop through each article tag and find all <a> tags inside it\n",
    "        for article in articles:\n",
    "            links = article.find_elements(By.TAG_NAME, 'a')\n",
    "            for link in links:\n",
    "                href = link.get_attribute('href')\n",
    "                if href:\n",
    "                    product_links.append(href)\n",
    "        \n",
    "        return product_links\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Loading the section took too long.\")\n",
    "        return []\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find the required elements on the page.\")\n",
    "        return []\n",
    "\n",
    "# Function to scrape product details from each product page\n",
    "def scrape_product_page(product_url):\n",
    "    driver.get(product_url)\n",
    "    \n",
    "    # Define the CSS selectors\n",
    "    h1_selector = 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-text.animation-fade-in > div > div > div.block-info-product__top.padding-line-element > a > h1'\n",
    "    img_selector = 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-img > div.animation-mask.p81 > div > a > img'\n",
    "    h3_selector = 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-text.animation-fade-in > div > div > h3'\n",
    "    div_selector_1 = '#block-0 > div > div'\n",
    "    div_selector_2 = '#block-2 > div > div'\n",
    "    img_selector_2 = '#block-4 > div > div > div.col-8.block-text-img-img.is-768 > div > img'\n",
    "    div_selector_3 = '#block-5 > div > div'\n",
    "    file_selector = '#specs > div > div.row.product-specs-row > div:nth-child(3) > a'\n",
    "\n",
    "    # Extract the <h1> text\n",
    "    h1_text = extract_text_from_selector(driver, h1_selector)\n",
    "    print(f\"Extracted <h1> text: {h1_text}\")\n",
    "\n",
    "    # Create directory based on <h1> text\n",
    "    h1_folder_name = h1_text.replace('/', '-').replace('\\\\', '-')  # Sanitize folder name\n",
    "    product_folder_path = os.path.join(h1_folder_name)\n",
    "    if not os.path.exists(product_folder_path):\n",
    "        os.makedirs(product_folder_path)\n",
    "\n",
    "    # Extract the image URL from the primary image selector\n",
    "    img_url = extract_image_url_from_selector(driver, img_selector)\n",
    "    print(f\"Extracted image URL: {img_url}\")\n",
    "\n",
    "    # Extract the <h3> text\n",
    "    h3_text = extract_text_from_selector(driver, h3_selector)\n",
    "    print(f\"Extracted <h3> text: {h3_text}\")\n",
    "\n",
    "    # Extract all image URLs from the specified divs\n",
    "    image_urls_div_1 = extract_all_images_from_div(driver, div_selector_1)\n",
    "    image_urls_div_2 = extract_all_images_from_div(driver, div_selector_2)\n",
    "    img_url_2 = extract_image_url_from_selector(driver, img_selector_2)\n",
    "    image_urls_div_3 = extract_all_images_from_div(driver, div_selector_3)\n",
    "\n",
    "    # List all image URLs to download\n",
    "    all_image_urls = [img_url] + [img_url_2] + image_urls_div_1 + image_urls_div_2 + image_urls_div_3\n",
    "    print(f\"All image URLs to download: {all_image_urls}\")\n",
    "\n",
    "    # Download images directly in the product folder\n",
    "    for idx, img_url in enumerate(all_image_urls):\n",
    "        if img_url and img_url.startswith('http'):\n",
    "            img_file_path = os.path.join(product_folder_path, f'image_{idx + 1}.png')\n",
    "            download_image(img_url, img_file_path)\n",
    "\n",
    "    # Extract file download link and download the file\n",
    "    file_element = attempt_element_locate(driver, file_selector)\n",
    "    if file_element:\n",
    "        file_link = file_element.get_attribute('href')\n",
    "        if file_link and file_link.startswith('http'):\n",
    "            download_file(file_link, os.path.join(product_folder_path, 'description.pdf'))\n",
    "        else:\n",
    "            print(\"No valid file link found.\")\n",
    "    else:\n",
    "        print(\"File element not found.\")\n",
    "\n",
    "# Helper functions for element extraction, image download, and file download\n",
    "def extract_text_from_selector(driver, css_selector, retries=5, wait_time=30):\n",
    "    try:\n",
    "        element = attempt_element_locate(driver, css_selector, retries, wait_time)\n",
    "        return element.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"Text not found\"\n",
    "\n",
    "def extract_image_url_from_selector(driver, css_selector, retries=5, wait_time=30):\n",
    "    try:\n",
    "        img_element = attempt_element_locate(driver, css_selector, retries, wait_time)\n",
    "        return img_element.get_attribute('src')\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting image URL: {e}\")\n",
    "        return \"Image URL not found\"\n",
    "\n",
    "def extract_all_images_from_div(driver, parent_css_selector, retries=5, wait_time=30):\n",
    "    try:\n",
    "        parent_div = attempt_element_locate(driver, parent_css_selector, retries, wait_time)\n",
    "        img_elements = parent_div.find_elements(By.TAG_NAME, 'img')\n",
    "        image_urls = [img.get_attribute('src') for img in img_elements if img.get_attribute('src')]\n",
    "        return image_urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting images from div: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_image(image_url, output_path):\n",
    "    try:\n",
    "        print(f\"Attempting to download image from: {image_url}\")\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(output_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"Image downloaded successfully: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "\n",
    "def download_file(file_url, output_path):\n",
    "    try:\n",
    "        print(f\"Attempting to download file from: {file_url}\")\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(output_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"File downloaded successfully: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "\n",
    "# Main script execution\n",
    "product_links = get_product_links()\n",
    "\n",
    "if product_links:\n",
    "    for product_link in product_links:\n",
    "        scrape_product_page(product_link)\n",
    "else:\n",
    "    print(\"No product links found.\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# URL of the page you want to scrape (all products page)\n",
    "main_url = 'https://www.molteni.it/ap/highlights'\n",
    "\n",
    "# Function to attempt element location with retries using CSS selector\n",
    "def attempt_element_locate(driver, selector, retries=5, wait_time=30):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            element = WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "            )\n",
    "            return element\n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            attempt += 1\n",
    "    raise Exception(f\"Failed to locate element with CSS Selector: {selector} after {retries} attempts\")\n",
    "\n",
    "# Function to extract all product links\n",
    "def get_product_links():\n",
    "    driver.get(main_url)\n",
    "    try:\n",
    "        # Wait until the section containing the articles is loaded\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        section = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section.product-category > div > div > div > section.content-block.catalog-list')))\n",
    "        \n",
    "        # Find all article tags within the section\n",
    "        articles = section.find_elements(By.TAG_NAME, 'article')\n",
    "        product_links = []\n",
    "        \n",
    "        # Loop through each article tag and find all <a> tags inside it\n",
    "        for article in articles:\n",
    "            links = article.find_elements(By.TAG_NAME, 'a')\n",
    "            for link in links:\n",
    "                href = link.get_attribute('href')\n",
    "                if href:\n",
    "                    product_links.append(href)\n",
    "        \n",
    "        return product_links\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Loading the section took too long.\")\n",
    "        return []\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find the required elements on the page.\")\n",
    "        return []\n",
    "\n",
    "# Function to scrape product details from each product page\n",
    "def scrape_product_page(product_url):\n",
    "    driver.get(product_url)\n",
    "    \n",
    "    # Define the CSS selectors\n",
    "    h1_selector = 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-text.animation-fade-in > div > div > div.block-info-product__top.padding-line-element > a > h1'\n",
    "    h3_selector = 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-text.animation-fade-in > div > div > h3'\n",
    "    file_selector = '#specs > div > div.row.product-specs-row > div:nth-child(3) > a'\n",
    "\n",
    "    # Extract the <h1> text\n",
    "    h1_text = extract_text_from_selector(driver, h1_selector)\n",
    "    print(f\"Extracted <h1> text: {h1_text}\")\n",
    "\n",
    "    # Sanitize folder name\n",
    "    h1_folder_name = h1_text.replace('/', '-').replace('\\\\', '-').replace(':', '-').replace('*', '-').replace('?', '-').replace('\"', '-').replace('<', '-').replace('>', '-').replace('|', '-')\n",
    "    product_folder_path = os.path.join(h1_folder_name)\n",
    "    print(f\"Creating directory at path: {product_folder_path}\")\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(product_folder_path):\n",
    "            os.makedirs(product_folder_path)\n",
    "        print(f\"Directory created successfully: {product_folder_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory: {e}\")\n",
    "        return\n",
    "\n",
    "    # Extract the <h3> text\n",
    "    h3_text = extract_text_from_selector(driver, h3_selector)\n",
    "    print(f\"Extracted <h3> text: {h3_text}\")\n",
    "\n",
    "    # Extract all image URLs from the entire page (limited to the first 18)\n",
    "    all_image_urls = extract_all_images_from_page(driver)\n",
    "    print(f\"All image URLs to download: {all_image_urls}\")\n",
    "\n",
    "    # Download images directly in the product folder\n",
    "    for idx, img_url in enumerate(all_image_urls[:18]):\n",
    "        if img_url and img_url.startswith('http'):\n",
    "            img_file_path = os.path.join(product_folder_path, f'image_{idx + 1}.png')\n",
    "            download_image(img_url, img_file_path)\n",
    "\n",
    "    # Extract file download link and download the file\n",
    "    file_element = attempt_element_locate(driver, file_selector)\n",
    "    if file_element:\n",
    "        file_link = file_element.get_attribute('href')\n",
    "        if file_link and file_link.startswith('http'):\n",
    "            download_file(file_link, os.path.join(product_folder_path, 'description.pdf'))\n",
    "        else:\n",
    "            print(\"No valid file link found.\")\n",
    "    else:\n",
    "        print(\"File element not found.\")\n",
    "\n",
    "# Helper functions for element extraction, image download, and file download\n",
    "def extract_text_from_selector(driver, css_selector, retries=5, wait_time=30):\n",
    "    try:\n",
    "        element = attempt_element_locate(driver, css_selector, retries, wait_time)\n",
    "        return element.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"Text not found\"\n",
    "\n",
    "def extract_all_images_from_page(driver, retries=5, wait_time=30):\n",
    "    try:\n",
    "        # Retrieve all img elements on the page\n",
    "        img_elements = driver.find_elements(By.TAG_NAME, 'img')\n",
    "        image_urls = [img.get_attribute('src') for img in img_elements if img.get_attribute('src')]\n",
    "        return image_urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting images from page: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_image(image_url, output_path):\n",
    "    try:\n",
    "        print(f\"Attempting to download image from: {image_url}\")\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(output_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"Image downloaded successfully: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "\n",
    "def download_file(file_url, output_path):\n",
    "    try:\n",
    "        print(f\"Attempting to download file from: {file_url}\")\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(output_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"File downloaded successfully: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "\n",
    "# Main script execution\n",
    "product_links = get_product_links()\n",
    "\n",
    "if product_links:\n",
    "    for product_link in product_links:\n",
    "        scrape_product_page(product_link)\n",
    "else:\n",
    "    print(\"No product links found.\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# URL of the page you want to scrape (all products page)\n",
    "main_url = 'https://www.molteni.it/ap/highlights'\n",
    "\n",
    "# Function to attempt element location with retries using CSS selector\n",
    "def attempt_element_locate(driver, selector, retries=5, wait_time=30):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            element = WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "            )\n",
    "            return element\n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            attempt += 1\n",
    "    raise Exception(f\"Failed to locate element with CSS Selector: {selector} after {retries} attempts\")\n",
    "\n",
    "# Function to extract all product links\n",
    "def get_product_links():\n",
    "    driver.get(main_url)\n",
    "    try:\n",
    "        # Wait until the section containing the articles is loaded\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        section = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section.product-category > div > div > div > section.content-block.catalog-list')))\n",
    "        \n",
    "        # Find all article tags within the section\n",
    "        articles = section.find_elements(By.TAG_NAME, 'article')\n",
    "        product_links = []\n",
    "        \n",
    "        # Loop through each article tag and find all <a> tags inside it\n",
    "        for article in articles:\n",
    "            links = article.find_elements(By.TAG_NAME, 'a')\n",
    "            for link in links:\n",
    "                href = link.get_attribute('href')\n",
    "                if href:\n",
    "                    product_links.append(href)\n",
    "        \n",
    "        return product_links\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Loading the section took too long.\")\n",
    "        return []\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find the required elements on the page.\")\n",
    "        return []\n",
    "\n",
    "# Function to scrape product details from each product page\n",
    "def scrape_product_page(product_url):\n",
    "    driver.get(product_url)\n",
    "    \n",
    "    # Define the CSS selectors\n",
    "    h1_selector = 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-text.animation-fade-in > div > div > div.block-info-product__top.padding-line-element > a > h1'\n",
    "    h3_selector = 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-text.animation-fade-in > div > div > h3'\n",
    "    file_selector = '#specs > div > div.row.product-specs-row > div:nth-child(3) > a'\n",
    "\n",
    "    # Extract the <h1> text\n",
    "    h1_text = extract_text_from_selector(driver, h1_selector)\n",
    "    print(f\"Extracted <h1> text: {h1_text}\")\n",
    "\n",
    "    # Sanitize folder name\n",
    "    h1_folder_name = h1_text.replace('/', '-').replace('\\\\', '-').replace(':', '-').replace('*', '-').replace('?', '-').replace('\"', '-').replace('<', '-').replace('>', '-').replace('|', '-')\n",
    "    product_folder_path = os.path.join(h1_folder_name)\n",
    "    print(f\"Creating directory at path: {product_folder_path}\")\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(product_folder_path):\n",
    "            os.makedirs(product_folder_path)\n",
    "        print(f\"Directory created successfully: {product_folder_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory: {e}\")\n",
    "        return\n",
    "\n",
    "    # Extract the <h3> text\n",
    "    h3_text = extract_text_from_selector(driver, h3_selector)\n",
    "    print(f\"Extracted <h3> text: {h3_text}\")\n",
    "\n",
    "    # Extract all image URLs from the page (limited to the first 18)\n",
    "    all_image_urls = extract_images_between_sections(driver, 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div', '#block-14 > div')\n",
    "    print(f\"All image URLs to download: {all_image_urls}\")\n",
    "\n",
    "    # Download images directly in the product folder\n",
    "    for idx, img_url in enumerate(all_image_urls[:18]):\n",
    "        if img_url and img_url.startswith('http'):\n",
    "            img_file_path = os.path.join(product_folder_path, f'image_{idx + 1}.png')\n",
    "            download_image(img_url, img_file_path)\n",
    "\n",
    "    # Extract file download link and download the file\n",
    "    file_element = attempt_element_locate(driver, file_selector)\n",
    "    if file_element:\n",
    "        file_link = file_element.get_attribute('href')\n",
    "        if file_link and file_link.startswith('http'):\n",
    "            download_file(file_link, os.path.join(product_folder_path, 'description.pdf'))\n",
    "        else:\n",
    "            print(\"No valid file link found.\")\n",
    "    else:\n",
    "        print(\"File element not found.\")\n",
    "\n",
    "# Helper functions for element extraction, image download, and file download\n",
    "def extract_text_from_selector(driver, css_selector, retries=5, wait_time=30):\n",
    "    try:\n",
    "        element = attempt_element_locate(driver, css_selector, retries, wait_time)\n",
    "        return element.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"Text not found\"\n",
    "\n",
    "def extract_images_between_sections(driver, start_selector, end_selector, retries=5, wait_time=30):\n",
    "    try:\n",
    "        # Retrieve the start and end elements\n",
    "        start_element = attempt_element_locate(driver, start_selector, retries, wait_time)\n",
    "        end_element = attempt_element_locate(driver, end_selector, retries, wait_time)\n",
    "\n",
    "        # Find all img elements on the page\n",
    "        img_elements = driver.find_elements(By.TAG_NAME, 'img')\n",
    "        image_urls = []\n",
    "        start_found = False\n",
    "        end_found = False\n",
    "\n",
    "        for img in img_elements:\n",
    "            # Check if the image is located within the start section\n",
    "            parent_element = img.find_element(By.XPATH, '..')  # Find the parent of the img\n",
    "            parent_id = parent_element.get_attribute('id')\n",
    "\n",
    "            if start_element in parent_element.find_elements(By.XPATH, '..'):\n",
    "                start_found = True\n",
    "\n",
    "            if start_found and not end_found:\n",
    "                # Add image URL to the list\n",
    "                image_url = img.get_attribute('src')\n",
    "                if image_url:\n",
    "                    image_urls.append(image_url)\n",
    "\n",
    "            if end_element in parent_element.find_elements(By.XPATH, '..'):\n",
    "                end_found = True\n",
    "                break\n",
    "\n",
    "        return image_urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting images from page: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_image(image_url, output_path):\n",
    "    try:\n",
    "        print(f\"Attempting to download image from: {image_url}\")\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(output_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"Image downloaded successfully: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "\n",
    "def download_file(file_url, output_path):\n",
    "    try:\n",
    "        print(f\"Attempting to download file from: {file_url}\")\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(output_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"File downloaded successfully: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "\n",
    "# Main script execution\n",
    "product_links = get_product_links()\n",
    "\n",
    "if product_links:\n",
    "    for product_link in product_links:\n",
    "        scrape_product_page(product_link)\n",
    "else:\n",
    "    print(\"No product links found.\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is working for one product only in second category \n",
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# URL of the page you want to scrape (specific product page)\n",
    "main_url = 'https://www.molteni.it/ap/product/intersection'\n",
    "\n",
    "# Open the webpage\n",
    "driver.get(main_url)\n",
    "\n",
    "def download_image(image_url, folder_path, image_name):\n",
    "    try:\n",
    "        # Get image content\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "        # Define the path where the image will be saved\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "\n",
    "        # Save the image\n",
    "        with open(image_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Saved image: {image_name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {image_url}. Reason: {e}\")\n",
    "\n",
    "def download_file(file_url, folder_path, file_name):\n",
    "    try:\n",
    "        # Get file content\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "        # Define the path where the file will be saved\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Save the file\n",
    "        with open(file_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Saved file: {file_name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {file_url}. Reason: {e}\")\n",
    "\n",
    "def extract_images_from_selector(selector, folder_path, image_counter):\n",
    "    try:\n",
    "        # Wait for the containers to be present\n",
    "        container_divs = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector))\n",
    "        )\n",
    "        \n",
    "        # Extract images from all containers\n",
    "        for container_div in container_divs:\n",
    "            # Find all images inside the container\n",
    "            all_images = container_div.find_elements(By.TAG_NAME, \"img\")\n",
    "            \n",
    "            # Loop through each found image and get the 'src' attribute\n",
    "            for img in all_images:\n",
    "                image_src = img.get_attribute('src')\n",
    "                image_name = f\"image_{image_counter}.png\"\n",
    "                download_image(image_src, folder_path, image_name)\n",
    "                image_counter += 1\n",
    "    \n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        print(f\"No images found inside the container {selector} or timeout occurred.\")\n",
    "    \n",
    "    return image_counter\n",
    "\n",
    "try:\n",
    "    # Extract text from h1 tag inside a specific div\n",
    "    text_selector = \"body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-text.animation-fade-in > div > div > div.block-info-product__top.padding-line-element\"\n",
    "    \n",
    "    try:\n",
    "        # Wait for the div to be present\n",
    "        info_div = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, text_selector))\n",
    "        )\n",
    "        \n",
    "        # Find the <a> tag inside the div\n",
    "        a_tag = info_div.find_element(By.TAG_NAME, \"a\")\n",
    "        \n",
    "        # Find the <h1> tag inside the <a> tag and get its text\n",
    "        h1_tag = a_tag.find_element(By.TAG_NAME, \"h1\")\n",
    "        h1_text = h1_tag.text.strip()\n",
    "        print(\"H1 Text:\", h1_text)\n",
    "        \n",
    "        # Create a directory with the H1 text as its name\n",
    "        folder_path = os.path.join(os.getcwd(), h1_text.replace('/', '_').replace('\\\\', '_'))  # Replace invalid characters\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        print(\"H1 tag not found inside the specified container or timeout occurred.\")\n",
    "        folder_path = os.getcwd()  # Use current directory if H1 text not found\n",
    "\n",
    "    # Extract images from all specified selectors and save to the created folder\n",
    "    selectors = [\n",
    "        \"body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1)\",\n",
    "        \"#block-0\",\n",
    "        \"#block-1\",\n",
    "        \"#block-2\",\n",
    "        \"#block-3\",\n",
    "        \"#block-4\",\n",
    "        \"#block-5\",\n",
    "        \"#block-6\",\n",
    "        \"#block-7\",\n",
    "        \"#block-8\",\n",
    "        \"#block-9\",\n",
    "        \"#block-10\",\n",
    "        \"#block-11\"\n",
    "    ]\n",
    "    \n",
    "    image_counter = 1\n",
    "    for selector in selectors:\n",
    "        image_counter = extract_images_from_selector(selector, folder_path, image_counter)\n",
    "\n",
    "    # Download the file from the specified anchor tag\n",
    "    anchor_selector = \"#specs > div > div.row.product-specs-row > div:nth-child(3) > a\"\n",
    "    \n",
    "    try:\n",
    "        # Wait for the anchor tag to be present\n",
    "        anchor_tag = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, anchor_selector))\n",
    "        )\n",
    "        \n",
    "        # Get the URL from the href attribute\n",
    "        file_url = anchor_tag.get_attribute('href')\n",
    "        file_name = \"description.pdf\"  # Set filename for the downloaded file\n",
    "        \n",
    "        # Download the file\n",
    "        download_file(file_url, folder_path, file_name)\n",
    "    \n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        print(\"Anchor tag not found inside the specified container or timeout occurred.\")\n",
    "\n",
    "except (NoSuchElementException, TimeoutException) as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser window\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is working for Kitchen category and will get all products\n",
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "def download_image(image_url, folder_path, image_name):\n",
    "    try:\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "        with open(image_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Saved image: {image_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {image_url}. Reason: {e}\")\n",
    "\n",
    "def download_file(file_url, folder_path, file_name):\n",
    "    try:\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Saved file: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {file_url}. Reason: {e}\")\n",
    "\n",
    "def extract_images_from_selector(selector, folder_path, image_counter):\n",
    "    try:\n",
    "        container_divs = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector))\n",
    "        )\n",
    "        for container_div in container_divs:\n",
    "            all_images = container_div.find_elements(By.TAG_NAME, \"img\")\n",
    "            for img in all_images:\n",
    "                image_src = img.get_attribute('src')\n",
    "                image_name = f\"image_{image_counter}.png\"\n",
    "                download_image(image_src, folder_path, image_name)\n",
    "                image_counter += 1\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        print(f\"No images found inside the container {selector} or timeout occurred.\")\n",
    "    return image_counter\n",
    "\n",
    "def get_product_links(base_url):\n",
    "    driver.get(base_url)\n",
    "    links = []\n",
    "    try:\n",
    "        product_elements = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section.product-category > div > div > div > section.content-block.catalog-list > article > a'))\n",
    "        )\n",
    "        for element in product_elements:\n",
    "            link = element.get_attribute('href')\n",
    "            if link:\n",
    "                links.append(link)\n",
    "    except (NoSuchElementException, TimeoutException) as e:\n",
    "        print(f\"An error occurred while fetching product links: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_product_page(product_url):\n",
    "    driver.get(product_url)\n",
    "    try:\n",
    "        text_selector = \"body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-text.animation-fade-in > div > div > div.block-info-product__top.padding-line-element\"\n",
    "        try:\n",
    "            info_div = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, text_selector))\n",
    "            )\n",
    "            a_tag = info_div.find_element(By.TAG_NAME, \"a\")\n",
    "            h1_tag = a_tag.find_element(By.TAG_NAME, \"h1\")\n",
    "            h1_text = h1_tag.text.strip()\n",
    "            print(\"H1 Text:\", h1_text)\n",
    "            folder_path = os.path.join(os.getcwd(), h1_text.replace('/', '_').replace('\\\\', '_'))\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(\"H1 tag not found inside the specified container or timeout occurred.\")\n",
    "            folder_path = os.getcwd()\n",
    "\n",
    "        selectors = [\n",
    "            \"body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1)\",\n",
    "            \"#block-0\",\n",
    "            \"#block-1\",\n",
    "            \"#block-2\",\n",
    "            \"#block-3\",\n",
    "            \"#block-4\",\n",
    "            \"#block-5\",\n",
    "            \"#block-6\",\n",
    "            \"#block-7\",\n",
    "            \"#block-8\",\n",
    "            \"#block-9\",\n",
    "            \"#block-10\",\n",
    "            \"#block-11\"\n",
    "        ]\n",
    "        image_counter = 1\n",
    "        for selector in selectors:\n",
    "            image_counter = extract_images_from_selector(selector, folder_path, image_counter)\n",
    "\n",
    "        anchor_selector = \"#specs > div > div.row.product-specs-row > div:nth-child(3) > a\"\n",
    "        try:\n",
    "            anchor_tag = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, anchor_selector))\n",
    "            )\n",
    "            file_url = anchor_tag.get_attribute('href')\n",
    "            file_name = \"description.pdf\"\n",
    "            download_file(file_url, folder_path, file_name)\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(\"Anchor tag not found inside the specified container or timeout occurred.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the product page {product_url}: {e}\")\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.molteni.it/ap/kitchens/category/highlights'\n",
    "    product_links = get_product_links(base_url)\n",
    "    for link in product_links:\n",
    "        scrape_product_page(link)\n",
    "\n",
    "except (NoSuchElementException, TimeoutException) as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "def download_image(image_url, folder_path, image_name):\n",
    "    try:\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "        with open(image_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Saved image: {image_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {image_url}. Reason: {e}\")\n",
    "\n",
    "def download_file(file_url, folder_path, file_name):\n",
    "    try:\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Saved file: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {file_url}. Reason: {e}\")\n",
    "\n",
    "def extract_images_from_selector(selector, folder_path, image_counter):\n",
    "    try:\n",
    "        container_divs = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector))\n",
    "        )\n",
    "        for container_div in container_divs:\n",
    "            all_images = container_div.find_elements(By.TAG_NAME, \"img\")\n",
    "            for img in all_images:\n",
    "                image_src = img.get_attribute('src')\n",
    "                image_name = f\"image_{image_counter}.png\"\n",
    "                download_image(image_src, folder_path, image_name)\n",
    "                image_counter += 1\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        print(f\"No images found inside the container {selector} or timeout occurred.\")\n",
    "    return image_counter\n",
    "\n",
    "def get_product_links(base_url):\n",
    "    driver.get(base_url)\n",
    "    links = []\n",
    "    try:\n",
    "        product_elements = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section.product-category > div > div > div > section.content-block.catalog-list > article > a'))\n",
    "        )\n",
    "        for element in product_elements:\n",
    "            link = element.get_attribute('href')\n",
    "            if link:\n",
    "                links.append(link)\n",
    "    except (NoSuchElementException, TimeoutException) as e:\n",
    "        print(f\"An error occurred while fetching product links: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_product_page(product_url):\n",
    "    driver.get(product_url)\n",
    "    try:\n",
    "        text_selector = \"body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1) > div > article > div.block-text-img-text.animation-fade-in > div > div > div.block-info-product__top.padding-line-element\"\n",
    "        try:\n",
    "            info_div = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, text_selector))\n",
    "            )\n",
    "            a_tag = info_div.find_element(By.TAG_NAME, \"a\")\n",
    "            h1_tag = a_tag.find_element(By.TAG_NAME, \"h1\")\n",
    "            h1_text = h1_tag.text.strip()\n",
    "            print(\"H1 Text:\", h1_text)\n",
    "            folder_path = os.path.join(os.getcwd(), h1_text.replace('/', '_').replace('\\\\', '_'))\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(\"H1 tag not found inside the specified container or timeout occurred.\")\n",
    "            folder_path = os.getcwd()\n",
    "\n",
    "        selectors = [\n",
    "            \"body > div.wrapper-site.avgrund-contents.no-ecommerce-bar > section:nth-child(1)\",\n",
    "            \"#block-0\",\n",
    "            \"#block-1\",\n",
    "            \"#block-2\",\n",
    "            \"#block-3\",\n",
    "            \"#block-4\",\n",
    "            \"#block-5\",\n",
    "            \"#block-6\",\n",
    "            \"#block-7\",\n",
    "            \"#block-8\",\n",
    "            \"#block-9\",\n",
    "            \"#block-10\",\n",
    "            \"#block-11\"\n",
    "        ]\n",
    "        image_counter = 1\n",
    "        for selector in selectors:\n",
    "            image_counter = extract_images_from_selector(selector, folder_path, image_counter)\n",
    "\n",
    "        anchor_selector = \"#specs > div > div.row.product-specs-row > div:nth-child(3) > a\"\n",
    "        try:\n",
    "            anchor_tag = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, anchor_selector))\n",
    "            )\n",
    "            file_url = anchor_tag.get_attribute('href')\n",
    "            file_name = \"description.pdf\"\n",
    "            download_file(file_url, folder_path, file_name)\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(\"Anchor tag not found inside the specified container or timeout occurred.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the product page {product_url}: {e}\")\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.molteni.it/ap/gio-ponti/category/highlights'\n",
    "    product_links = get_product_links(base_url)\n",
    "    for link in product_links:\n",
    "        scrape_product_page(link)\n",
    "\n",
    "except (NoSuchElementException, TimeoutException) as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove any characters that are invalid in Windows file names and trim whitespace\n",
    "    filename = re.sub(r'[<>:\"/\\\\|?*\\n\\r]', '', filename).strip()\n",
    "    return filename\n",
    "\n",
    "def download_image(image_url, folder_path, image_name):\n",
    "    try:\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "        with open(image_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Saved image: {image_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {image_url}. Reason: {e}\")\n",
    "\n",
    "def download_file(file_url, folder_path, file_name):\n",
    "    try:\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Saved file: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {file_url}. Reason: {e}\")\n",
    "\n",
    "def extract_images_from_selector(selector, folder_path, image_counter):\n",
    "    try:\n",
    "        container_divs = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector))\n",
    "        )\n",
    "        for container_div in container_divs:\n",
    "            all_images = container_div.find_elements(By.TAG_NAME, \"img\")\n",
    "            for img in all_images:\n",
    "                image_src = img.get_attribute('src')\n",
    "                if image_src:\n",
    "                    image_name = f\"image_{image_counter}.png\"\n",
    "                    download_image(image_src, folder_path, image_name)\n",
    "                    image_counter += 1\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        print(f\"No images found inside the container {selector} or timeout occurred.\")\n",
    "    return image_counter\n",
    "\n",
    "def get_product_links(base_url):\n",
    "    driver.get(base_url)\n",
    "    links = []\n",
    "    try:\n",
    "        # Use the provided selector to locate the container with multiple tags\n",
    "        container_selector = \"#c25957 > div > section.container.prv-list.show-sofa-1\"\n",
    "        \n",
    "        # Wait for the container to be present and extract anchor elements from it\n",
    "        container = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, container_selector))\n",
    "        )\n",
    "        \n",
    "        # Find all <a> tags within the container\n",
    "        product_elements = container.find_elements(By.TAG_NAME, 'a')\n",
    "        \n",
    "        # Extract the URLs from the href attributes of each <a> tag\n",
    "        for element in product_elements:\n",
    "            link = element.get_attribute('href')\n",
    "            if link:\n",
    "                links.append(link)\n",
    "                print(f\"Product link found: {link}\")\n",
    "    except (NoSuchElementException, TimeoutException) as e:\n",
    "        print(f\"An error occurred while fetching product links: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_product_page(product_url):\n",
    "    driver.get(product_url)\n",
    "    try:\n",
    "            \n",
    "        # Use the new H1 selector for folder naming\n",
    "        h1_selector = \"#c205373 > div > section > div > div > h1\"\n",
    "        try:\n",
    "            h1_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, h1_selector))\n",
    "            )\n",
    "            h1_text = h1_element.text.strip()\n",
    "            print(\"H1 Text (Folder Name):\", h1_text)\n",
    "            folder_name = sanitize_filename(h1_text)\n",
    "            folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(\"H1 tag not found or timeout occurred.\")\n",
    "            folder_path = os.getcwd()  # Default to current directory\n",
    "\n",
    "        # New selectors for images\n",
    "        selectors = [\n",
    "            \"#c206647\",  # First container with multiple tags with images\n",
    "            \"body > main > section:nth-child(8)\",  # Second container with multiple divs containing images\n",
    "            \"body > main > section:nth-child(9)\"   # Third container with multiple divs containing images\n",
    "        ]\n",
    "        \n",
    "        # Extract images from new selectors\n",
    "        image_counter = 1\n",
    "        for selector in selectors:\n",
    "            image_counter = extract_images_from_selector(selector, folder_path, image_counter)\n",
    "\n",
    "        # New selector for file download\n",
    "        file_selector = \"#c205373 > div > section > div > div > ul:nth-child(4) > li:nth-child(2) > a\"\n",
    "        try:\n",
    "            anchor_tag = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, file_selector))\n",
    "            )\n",
    "            file_url = anchor_tag.get_attribute('href')\n",
    "            file_name = \"description.pdf\"\n",
    "            download_file(file_url, folder_path, file_name)\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(\"File download link not found or timeout occurred.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the product page {product_url}: {e}\")\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.rolf-benz.com/en_OC/furniture/sofas'\n",
    "    product_links = get_product_links(base_url)\n",
    "    print(f\"Total product links found: {len(product_links)}\")\n",
    "    \n",
    "    # Visit each product page and scrape the required data\n",
    "    for link in product_links:\n",
    "        scrape_product_page(link)\n",
    "\n",
    "except (NoSuchElementException, TimeoutException) as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/sina\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/moyo\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/mioko\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/rolf-benz-kumo\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/cara\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/ego-1\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/jola\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/linea\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/liv\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/mera\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/nuvola\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/onda\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/plura\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/volo\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/50\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/310\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/322\n",
      "Product link found: https://www.rolf-benz.com/en_OC/furniture/sofas/6500\n",
      "Total product links found: 18\n",
      "Extracted text: rolf-benz sina\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz moyo\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz mioko\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz-kumo\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz cara\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz ego-1\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz jola\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz linea\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz liv\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz mera\n",
      "File not found using any provided XPaths.\n",
      "Extracted text: rolf-benz nuvola\n",
      "File not found using any provided XPaths.\n",
      "Extracted text: rolf-benz onda\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz plura\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz volo\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz 50\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz 310\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz 322\n",
      "Saved file: description.pdf\n",
      "Extracted text: rolf-benz 6500\n",
      "Saved file: description.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "def get_product_links(base_url):\n",
    "    driver.get(base_url)\n",
    "    links = []\n",
    "    try:\n",
    "        # Use the provided selector to locate the container with multiple tags\n",
    "        container_selector = \"#c25957 > div > section.container.prv-list.show-sofa-1\"\n",
    "        \n",
    "        # Wait for the container to be present and extract anchor elements from it\n",
    "        container = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, container_selector))\n",
    "        )\n",
    "        \n",
    "        # Find all <a> tags within the container\n",
    "        product_elements = container.find_elements(By.TAG_NAME, 'a')\n",
    "        \n",
    "        # Extract the URLs from the href attributes of each <a> tag\n",
    "        for element in product_elements:\n",
    "            link = element.get_attribute('href')\n",
    "            if link:\n",
    "                links.append(link)\n",
    "                print(f\"Product link found: {link}\")\n",
    "    except (NoSuchElementException, TimeoutException) as e:\n",
    "        print(f\"An error occurred while fetching product links: {e}\")\n",
    "    return links\n",
    "\n",
    "def download_file(file_url, folder_path, file_name):\n",
    "    try:\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Saved file: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {file_url}. Reason: {e}\")\n",
    "\n",
    "def extract_and_save_product_info(url):\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        # Extract text after \"sofas/\"\n",
    "        if \"sofas/\" in url:\n",
    "            text_after_sofas = url.split(\"sofas/\")[1]\n",
    "            # Add \"rolf-benz\" if not present\n",
    "            if \"rolf-benz\" not in text_after_sofas:\n",
    "                text_after_sofas = f\"rolf-benz {text_after_sofas}\"\n",
    "            print(f\"Extracted text: {text_after_sofas}\")\n",
    "            \n",
    "            # Create a folder with the extracted text\n",
    "            folder_name = text_after_sofas.replace('/', '_').replace('\\\\', '_')\n",
    "            folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            # List of XPaths to try for downloading the file\n",
    "            file_xpaths = [\n",
    "                \"/html/body/main/div[9]/section/div/div/ul/li[1]/a\",\n",
    "                \"/html/body/main/div[5]/div/section/div/div/ul[1]/li[2]/a\",\n",
    "                \"/html/body/main/div[2]/div/section/div/div/ul[1]/li[1]/a\",\n",
    "                \"/html/body/main/div[2]/div/section/div/div/ul[1]/li[2]/a\",\n",
    "                \"/html/body/main/div[8]/section/div/div/ul/li[1]/a\",\n",
    "                \"/html/body/main/div[2]/div/section/div/div/ul[1]/li[1]/a\",\n",
    "                \"/html/body/main/div[8]/section/div/div/ul/li/a\",\n",
    "                \"/html/body/main/div[6]/div/section/div/div/ul[1]/li[2]/a\",\n",
    "                \"/html/body/main/div[2]/div/section/div/div/ul[1]/li/a\",\n",
    "                \"/html/body/main/div[8]/section/div/div/ul/li[1]/a\"\n",
    "            ]\n",
    "            \n",
    "            file_url = None\n",
    "            for xpath in file_xpaths:\n",
    "                try:\n",
    "                    file_element = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, xpath))\n",
    "                    )\n",
    "                    file_url = file_element.get_attribute('href')\n",
    "                    if file_url:\n",
    "                        break\n",
    "                except (NoSuchElementException, TimeoutException):\n",
    "                    continue\n",
    "            \n",
    "            if file_url:\n",
    "                download_file(file_url, folder_path, 'description.pdf')\n",
    "            else:\n",
    "                print(\"File not found using any provided XPaths.\")\n",
    "                \n",
    "        else:\n",
    "            print(\"URL does not contain 'sofas/'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the URL {url}: {e}\")\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.rolf-benz.com/en_OC/furniture/sofas'\n",
    "    product_links = get_product_links(base_url)\n",
    "    print(f\"Total product links found: {len(product_links)}\")\n",
    "    \n",
    "    # Visit each product page and process the information\n",
    "    for link in product_links:\n",
    "        extract_and_save_product_info(link)\n",
    "\n",
    "except (NoSuchElementException, TimeoutException) as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
