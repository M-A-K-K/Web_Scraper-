{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no browser window)\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://www.daraz.pk/products/5g-a35-8-gb-256-gb-13-mp-50-mp-8-mp-5-mp-5000-mah-i495188891-s2365989250.html'\n",
    "\n",
    "try:\n",
    "    # Open the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load (adjust time if needed)\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate and extract information from the image\n",
    "    image_element = driver.find_element(By.XPATH, '//*[@id=\"module_item_gallery_1\"]/div/div[1]/div[1]/img')\n",
    "    image_src = image_element.get_attribute('src')\n",
    "    image_alt = image_element.get_attribute('alt')\n",
    "    image_title = image_element.get_attribute('title')\n",
    "    image_width = image_element.get_attribute('width')\n",
    "    image_height = image_element.get_attribute('height')\n",
    "\n",
    "    print(f\"Image source URL: {image_src}\")\n",
    "    print(f\"Image alt text: {image_alt}\")\n",
    "    print(f\"Image title text: {image_title}\")\n",
    "    print(f\"Image width: {image_width}\")\n",
    "    print(f\"Image height: {image_height}\")\n",
    "\n",
    "    # Locate and extract data from additional XPaths\n",
    "    # Data from first XPath\n",
    "    \n",
    "    # Data from second XPath\n",
    "    data2 = driver.find_element(By.XPATH, '//*[@id=\"module_product_review\"]/div/div/div[3]/div/div').text\n",
    "    print(f\"Data from XPath 2: {data2}\")\n",
    "\n",
    "    # Data from third XPath\n",
    "    data3 = driver.find_element(By.XPATH, '//*[@id=\"module_product_detail\"]/div/div/div[1]/div[3]/div[1]').text\n",
    "    print(f\"Data from XPath 3: {data3}\")\n",
    "\n",
    "    # Data from fourth XPath\n",
    "    data4 = driver.find_element(By.XPATH, '//*[@id=\"block-qL431vav99\"]').text\n",
    "    print(f\"Data from XPath 4: {data4}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import time\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run in headless mode\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://www.daraz.pk/products/5g-a35-8-gb-256-gb-13-mp-50-mp-8-mp-5-mp-5000-mah-i495188891-s2365989250.html'\n",
    "\n",
    "def attempt_element_locate(xpath, retries=5, wait_time=15):\n",
    "    \"\"\" Attempt to locate an element with retries. \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            element = WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.XPATH, xpath))\n",
    "            )\n",
    "            return element\n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "    raise Exception(f\"Failed to locate element with XPath: {xpath} after {retries} attempts\")\n",
    "\n",
    "try:\n",
    "    # Open the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Locate the parent element (if needed)\n",
    "    parent_xpath = '/html/body/div[4]/div/div[10]'\n",
    "    parent_element = attempt_element_locate(parent_xpath)\n",
    "\n",
    "    # Locate the child element with full XPath\n",
    "    child_xpath = '/html/body/div[4]/div/div[10]/div[1]/div[1]/div/div/div/div[1]/div[2]/article/p[2]/span'\n",
    "    child_element = attempt_element_locate(child_xpath)\n",
    "\n",
    "    # Extract and print the text from the child element\n",
    "    child_text = child_element.text\n",
    "    print(f\"Text from child element: {child_text}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import time\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run in headless mode\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://www.daraz.pk/products/5g-a35-8-gb-256-gb-13-mp-50-mp-8-mp-5-mp-5000-mah-i495188891-s2365989250.html'\n",
    "\n",
    "def attempt_element_locate(xpath, retries=4, wait_time=5):\n",
    "    \"\"\" Attempt to locate an element with retries. \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            element = WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.XPATH, xpath))\n",
    "            )\n",
    "            return element\n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "    raise Exception(f\"Failed to locate element with XPath: {xpath} after {retries} attempts\")\n",
    "\n",
    "def extract_images_from_xpath(parent_xpath, retries=5, wait_time=15):\n",
    "    \"\"\" Function to extract all image sources from a given parent XPath. \"\"\"\n",
    "    try:\n",
    "        # Locate the parent element containing images\n",
    "        parent_element = attempt_element_locate(parent_xpath, retries, wait_time)\n",
    "        \n",
    "        # Find all the child elements that contain images\n",
    "        image_elements = parent_element.find_elements(By.TAG_NAME, 'img')\n",
    "\n",
    "        # Extract all image sources\n",
    "        image_sources = [img.get_attribute('src') for img in image_elements]\n",
    "        return image_sources\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting images: {e}\")\n",
    "        return []\n",
    "\n",
    "try:\n",
    "    # Open the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(15)\n",
    "\n",
    "    # Extract and print the text from a specific child element\n",
    "    child_xpath = '/html/body/div[4]/div/div[10]/div[1]/div[1]/div/div/div/div[1]/div[2]/article/p[2]/span'\n",
    "    child_element = attempt_element_locate(child_xpath)\n",
    "    child_text = child_element.text\n",
    "    print(f\"Text from child element: {child_text}\")\n",
    "\n",
    "    # Extract and print all image sources from the parent and child XPaths\n",
    "    parent_xpath = '/html/body/div[4]/div/div[3]/div[1]/div/div/div[2]'\n",
    "    image_sources = extract_images_from_xpath(parent_xpath)\n",
    "\n",
    "    # Print all the image sources\n",
    "    if image_sources:\n",
    "        print(\"Image sources found:\")\n",
    "        for img_src in image_sources:\n",
    "            print(img_src)\n",
    "    else:\n",
    "        print(\"No image sources found.\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "#chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://www.daraz.pk/smartphones/?up_id=100922545&clickTrackInfo=98daa0ef-574a-44ef-9dff-cd4913f7490a__3__100922545__static__0.1__158061__7253&from=hp_categories&item_id=100922545&version=v2&params=%7B%22catIdLv1%22%3A%222%22%2C%22pvid%22%3A%2298daa0ef-574a-44ef-9dff-cd4913f7490a%22%2C%22src%22%3A%22hp_categories%22%2C%22categoryName%22%3A%22Mobiles%22%2C%22categoryId%22%3A%223%22%7D&src=hp_categories&spm=a2a0e.tm80335159.categoriesPC.d_4_3'\n",
    "\n",
    "try:\n",
    "    # Open the webpage\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the page to load completely\n",
    "    time.sleep(10)  # Adjust sleep time if necessary\n",
    "\n",
    "    # Find the parent element containing all products\n",
    "    parent_xpath = '/html/body/div[3]/div/div[2]/div[1]/div/div[1]'\n",
    "    parent_element = driver.find_element(By.XPATH, parent_xpath)\n",
    "\n",
    "    # Find all anchor tags (<a>) within the parent element that contain links to product pages\n",
    "    child_xpath = './/a'  # Generalized to find all <a> tags inside the parent container\n",
    "    link_elements = parent_element.find_elements(By.XPATH, child_xpath)\n",
    "\n",
    "    # Extract and print the href attribute for each anchor tag\n",
    "    product_links = []\n",
    "    for link in link_elements:\n",
    "        product_href = link.get_attribute('href')\n",
    "        if product_href:\n",
    "            product_links.append(product_href)\n",
    "            print(f\"Product URL: {product_href}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Output all the product links\n",
    "print(\"All product page URLs:\")\n",
    "for idx, product_link in enumerate(product_links):\n",
    "    print(f\"{idx+1}: {product_link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is wokring best \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import time\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "#chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://www.daraz.pk/smartphones/?up_id=100922545&clickTrackInfo=98daa0ef-574a-44ef-9dff-cd4913f7490a__3__100922545__static__0.1__158061__7253&from=hp_categories&item_id=100922545&version=v2&params=%7B%22catIdLv1%22%3A%222%22%2C%22pvid%22%3A%2298daa0ef-574a-44ef-9dff-cd4913f7490a%22%2C%22src%22%3A%22hp_categories%22%2C%22categoryName%22%3A%22Mobiles%22%2C%22categoryId%22%3A%223%22%7D&src=hp_categories&spm=a2a0e.tm80335159.categoriesPC.d_4_3'\n",
    "\n",
    "# Function to attempt element location with retries\n",
    "def attempt_element_locate(xpath, retries=5, wait_time=15):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            element = WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.XPATH, xpath))\n",
    "            )\n",
    "            return element\n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "    raise Exception(f\"Failed to locate element with XPath: {xpath} after {retries} attempts\")\n",
    "\n",
    "try:\n",
    "    # Open the product listing webpage\n",
    "    driver.get(url)\n",
    "    time.sleep(10)  # Allow the page to load\n",
    "\n",
    "    # Find the parent element containing all products\n",
    "    parent_xpath = '/html/body/div[3]/div/div[2]/div[1]/div/div[1]'\n",
    "    parent_element = driver.find_element(By.XPATH, parent_xpath)\n",
    "\n",
    "    # Find all anchor tags (<a>) within the parent element that contain links to product pages\n",
    "    child_xpath = './/a'\n",
    "    link_elements = parent_element.find_elements(By.XPATH, child_xpath)\n",
    "\n",
    "    # Extract href (links to individual product pages)\n",
    "    product_links = []\n",
    "    for link in link_elements:\n",
    "        product_href = link.get_attribute('href')\n",
    "        if product_href:\n",
    "            product_links.append(product_href)\n",
    "\n",
    "    print(f\"Found {len(product_links)} product links. Visiting each product page...\")\n",
    "\n",
    "    # Visit each product page and extract required information\n",
    "    for idx, product_link in enumerate(product_links):\n",
    "        print(f\"\\nOpening product page {idx + 1}: {product_link}\")\n",
    "        driver.get(product_link)\n",
    "        time.sleep(10)  # Let the product page load\n",
    "\n",
    "        try:\n",
    "            # Locate and extract information from the image\n",
    "            image_xpath = '//*[@id=\"module_item_gallery_1\"]/div/div[1]/div[1]/img'\n",
    "            image_element = attempt_element_locate(image_xpath)\n",
    "\n",
    "            image_src = image_element.get_attribute('src')\n",
    "            image_alt = image_element.get_attribute('alt')\n",
    "            image_title = image_element.get_attribute('title')\n",
    "            image_width = image_element.get_attribute('width')\n",
    "            image_height = image_element.get_attribute('height')\n",
    "\n",
    "            print(f\"Image source URL: {image_src}\")\n",
    "            print(f\"Image alt text: {image_alt}\")\n",
    "            print(f\"Image title text: {image_title}\")\n",
    "            print(f\"Image width: {image_width}\")\n",
    "            print(f\"Image height: {image_height}\")\n",
    "            \n",
    "            # Extract product description using the updated XPath\n",
    "            try:\n",
    "                description_xpath = '/html/body/div[4]/div/div[10]/div[1]/div[1]/div/div'\n",
    "                description_element = attempt_element_locate(description_xpath)\n",
    "                description_text = description_element.text\n",
    "                print(f\"Product Description: {description_text}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to get product description: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get image or other information from product page {idx + 1}: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import time\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://www.daraz.pk/smartphones/?up_id=100922545&clickTrackInfo=98daa0ef-574a-44ef-9dff-cd4913f7490a__3__100922545__static__0.1__158061__7253&from=hp_categories&item_id=100922545&version=v2&params=%7B%22catIdLv1%22%3A%222%22%2C%22pvid%22%3A%2298daa0ef-574a-44ef-9dff-cd4913f7490a%22%2C%22src%22%3A%22hp_categories%22%2C%22categoryName%22%3A%22Mobiles%22%2C%22categoryId%22%3A%223%22%7D&src=hp_categories&spm=a2a0e.tm80335159.categoriesPC.d_4_3'\n",
    "\n",
    "# Function to attempt element location with retries\n",
    "def attempt_element_locate(xpath, retries=5, wait_time=15):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            element = WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.XPATH, xpath))\n",
    "            )\n",
    "            return element\n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "    raise Exception(f\"Failed to locate element with XPath: {xpath} after {retries} attempts\")\n",
    "\n",
    "# Function to attempt URL loading with retries\n",
    "def attempt_url_load(url, retries=5, wait_time=15):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(wait_time)  # Allow time for page to load\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} to load URL {url} failed: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "    raise Exception(f\"Failed to load URL: {url} after {retries} attempts\")\n",
    "\n",
    "# Directory to save the product details\n",
    "output_dir = 'product_details'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Open the product listing webpage\n",
    "attempt_url_load(url)\n",
    "\n",
    "# Find the parent element containing all products\n",
    "parent_xpath = '/html/body/div[3]/div/div[2]/div[1]/div/div[1]'\n",
    "parent_element = attempt_element_locate(parent_xpath)\n",
    "\n",
    "# Find all anchor tags (<a>) within the parent element that contain links to product pages\n",
    "child_xpath = './/a'\n",
    "link_elements = parent_element.find_elements(By.XPATH, child_xpath)\n",
    "\n",
    "# Extract href (links to individual product pages)\n",
    "product_links = []\n",
    "for link in link_elements:\n",
    "    product_href = link.get_attribute('href')\n",
    "    if product_href:\n",
    "        product_links.append(product_href)\n",
    "\n",
    "# Save all product links to a file\n",
    "links_file_path = os.path.join(output_dir, 'product_links.txt')\n",
    "with open(links_file_path, 'w') as file:\n",
    "    for product_link in product_links:\n",
    "        file.write(f\"{product_link}\\n\")\n",
    "\n",
    "print(f\"Saved {len(product_links)} product links to {links_file_path}. Visiting each product page...\")\n",
    "\n",
    "# Visit each product page and extract required information\n",
    "for idx, product_link in enumerate(product_links):\n",
    "    print(f\"\\nOpening product page {idx + 1}: {product_link}\")\n",
    "    \n",
    "    # Attempt to load the product page\n",
    "    attempt_url_load(product_link)\n",
    "\n",
    "    try:\n",
    "        # Locate and extract information from the image\n",
    "        image_xpath = '//*[@id=\"module_item_gallery_1\"]/div/div[1]/div[1]/img'\n",
    "        image_element = attempt_element_locate(image_xpath)\n",
    "\n",
    "        image_src = image_element.get_attribute('src')\n",
    "        image_alt = image_element.get_attribute('alt')\n",
    "        image_title = image_element.get_attribute('title')\n",
    "        image_width = image_element.get_attribute('width')\n",
    "        image_height = image_element.get_attribute('height')\n",
    "\n",
    "        # Extract product description using the updated XPath\n",
    "        try:\n",
    "            description_xpath = '/html/body/div[4]/div/div[10]/div[1]/div[1]/div/div'\n",
    "            description_element = attempt_element_locate(description_xpath)\n",
    "            description_text = description_element.text\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get product description: {e}\")\n",
    "            description_text = \"Description not found\"\n",
    "\n",
    "        # Create a file for each product with its details\n",
    "        product_file_path = os.path.join(output_dir, f'product_{idx + 1}.txt')\n",
    "        with open(product_file_path, 'w') as file:\n",
    "            file.write(f\"Product URL: {product_link}\\n\\n\")\n",
    "            file.write(f\"Image Source URL: {image_src}\\n\")\n",
    "            file.write(f\"Image Alt Text: {image_alt}\\n\")\n",
    "            file.write(f\"Image Title Text: {image_title}\\n\")\n",
    "            file.write(f\"Image Width: {image_width}\\n\")\n",
    "            file.write(f\"Image Height: {image_height}\\n\\n\")\n",
    "            file.write(f\"Product Description: {description_text}\\n\")\n",
    "\n",
    "        print(f\"Saved details of product {idx + 1} to {product_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get image or other information from product page {idx + 1}: {e}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 40 unique, filtered product links to product_details\\product_links.txt. Visiting each product page...\n",
      "\n",
      "Opening product page 1: https://www.daraz.pk/products/v30-rs-ips-lcd-675-4gb-128gb-5mp-108mp-5000mah-i537343371.html\n",
      "Saved details of product 1 to product_details\\product_1.txt\n",
      "\n",
      "Opening product page 2: https://www.daraz.pk/products/6gb-c61-128gb-50mp-ai-90hz-5000mah-i553513335.html\n",
      "Saved details of product 2 to product_details\\product_2.txt\n",
      "\n",
      "Opening product page 3: https://www.daraz.pk/products/9-8gb-12gb-9-gb-128-5-hz-i518694077.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import time\n",
    "\n",
    "# Path to your WebDriver executable\n",
    "webdriver_path = 'D:/Internship_Developers_den/web_scraping/folder_driver/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment if you want to run it headlessly\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(webdriver_path), options=chrome_options)\n",
    "\n",
    "# Function to attempt element location with retries\n",
    "def attempt_element_locate(xpath, retries=5, wait_time=25):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            element = WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.XPATH, xpath))\n",
    "            )\n",
    "            return element\n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "    raise Exception(f\"Failed to locate element with XPath: {xpath} after {retries} attempts\")\n",
    "\n",
    "# Function to extract all images from a given parent XPath\n",
    "def extract_images_from_xpath(parent_xpath, retries=5, wait_time=25):\n",
    "    try:\n",
    "        # Locate the parent element containing images\n",
    "        parent_element = attempt_element_locate(parent_xpath, retries, wait_time)\n",
    "        \n",
    "        # Find all image elements within the parent element\n",
    "        image_elements = parent_element.find_elements(By.TAG_NAME, 'img')\n",
    "\n",
    "        # Extract all relevant image information\n",
    "        images = []\n",
    "        for img in image_elements:\n",
    "            # Try different attributes since images may be lazy-loaded\n",
    "            img_src = img.get_attribute('src') or img.get_attribute('data-src') or img.get_attribute('data-lazy-src')\n",
    "            images.append({\n",
    "                'src': img_src,\n",
    "                'alt': img.get_attribute('alt'),\n",
    "                'title': img.get_attribute('title'),\n",
    "                'width': img.get_attribute('width'),\n",
    "                'height': img.get_attribute('height'),\n",
    "            })\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting images: {e}\")\n",
    "        return []\n",
    "\n",
    "# Directory to save the product details\n",
    "output_dir = 'product_details'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://www.daraz.pk/smartphones/?up_id=100922545&clickTrackInfo=98daa0ef-574a-44ef-9dff-cd4913f7490a__3__100922545__static__0.1__158061__7253&from=hp_categories&item_id=100922545&version=v2&params=%7B%22catIdLv1%22%3A%222%22%2C%22pvid%22%3A%2298daa0ef-574a-44ef-9dff-cd4913f7490a%22%2C%22src%22%3A%22hp_categories%22%2C%22categoryName%22%3A%22Mobiles%22%2C%22categoryId%22%3A%223%22%7D&src=hp_categories&spm=a2a0e.tm80335159.categoriesPC.d_4_3'\n",
    "\n",
    "# Open the product listing webpage\n",
    "driver.get(url)\n",
    "time.sleep(25)  # Wait for the page to fully load\n",
    "\n",
    "# Find the parent element containing all products\n",
    "parent_xpath = '/html/body/div[3]/div/div[2]'  # Updated XPath\n",
    "parent_element = attempt_element_locate(parent_xpath)\n",
    "\n",
    "# Find all anchor tags (<a>) within the parent element that contain links to product pages\n",
    "child_xpath = './/a'\n",
    "link_elements = parent_element.find_elements(By.XPATH, child_xpath)\n",
    "\n",
    "product_links = set()  # Use a set to store unique links\n",
    "for link in link_elements:\n",
    "    product_href = link.get_attribute('href')\n",
    "    if product_href:\n",
    "        # Only add the link if it doesn't start with \"https://www.daraz.pk/smartphones\"\n",
    "        if not product_href.startswith(\"https://www.daraz.pk/smartphones\"):\n",
    "            product_links.add(product_href)  # Add to set, duplicates will be ignored\n",
    "\n",
    "# Save unique, filtered product links to a file\n",
    "links_file_path = os.path.join(output_dir, 'product_links.txt')\n",
    "with open(links_file_path, 'w') as file:\n",
    "    for product_link in product_links:\n",
    "        file.write(f\"{product_link}\\n\")\n",
    "\n",
    "print(f\"Saved {len(product_links)} unique, filtered product links to {links_file_path}. Visiting each product page...\")\n",
    "\n",
    "# Visit each product page and extract required information\n",
    "for idx, product_link in enumerate(product_links):\n",
    "    print(f\"\\nOpening product page {idx + 1}: {product_link}\")\n",
    "    \n",
    "    # Attempt to load the product page\n",
    "    driver.get(product_link)\n",
    "    time.sleep(25)  # Allow time for page to load\n",
    "\n",
    "    try:\n",
    "        # Locate and extract all images from the specified gallery XPath\n",
    "        gallery_xpath = '/html/body/div[4]/div/div[3]/div[1]/div/div/div[2]/div'  # Provided XPath for image gallery\n",
    "        image_gallery = extract_images_from_xpath(gallery_xpath)\n",
    "\n",
    "        # Extract product description (optional, as per your requirement)\n",
    "        try:\n",
    "            description_xpath = '/html/body/div[4]/div/div[10]/div[1]/div[1]/div/div'\n",
    "            description_element = attempt_element_locate(description_xpath)\n",
    "            description_text = description_element.text\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get product description: {e}\")\n",
    "            description_text = \"Description not found\"\n",
    "\n",
    "        # Create a file for each product with its details\n",
    "        product_file_path = os.path.join(output_dir, f'product_{idx + 1}.txt')\n",
    "        with open(product_file_path, 'w') as file:\n",
    "            file.write(f\"Product URL: {product_link}\\n\\n\")\n",
    "            file.write(f\"Product Description: {description_text}\\n\\n\")\n",
    "            \n",
    "            # Save details of all images in the gallery\n",
    "            file.write(f\"Image Gallery:\\n\")\n",
    "            for img in image_gallery:\n",
    "                file.write(f\"- Image Source: {img['src']}, Alt: {img['alt']}, Title: {img['title']}, Size: {img['width']}x{img['height']}\\n\")\n",
    "\n",
    "        print(f\"Saved details of product {idx + 1} to {product_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get image or other information from product page {idx + 1}: {e}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
